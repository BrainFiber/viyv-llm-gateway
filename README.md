# viyv-llm-gateway

<p align="center">
  <img src="https://raw.githubusercontent.com/BrainFiber/.assets/main/viyv.svg" width="120" alt="viyv logo"/>
</p>

Edgeâ€‘friendly reverse proxy that hides your LLM API keys and lets you route requests to **OpenAI / GoogleÂ Gemini / Anthropic** (or any other provider) with a single, stable URL.

---

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)Â 
[![Bun](https://img.shields.io/badge/Bun-v1.1+-black?logo=bun)](https://bun.sh)Â 
[![CI](https://github.com/BrainFiber/viyv-llm-gateway/actions/workflows/ci.yml/badge.svg)](../../actions/workflows/ci.yml)

---

## âœ¨ Features

| Capability | Details |
|-------------|---------|
| **ğŸ”‘ Key injection** | Your client never sees the real provider key â€“Â the gateway adds it at runtime. |
| **ğŸ“¡ Multiâ€‘provider** | Builtâ€‘in adapters for `openai`, `gemini`, `anthropic`. Add more in **1Â block of YAML/TypeScript**. |
| **âš¡ Edgeâ€‘ready** | Runs on Bun, NodeÂ 18+, CloudflareÂ Workers, Deno, Vercel Edge â€“Â same code, no tweak. |
| **ğŸŒ€ Streaming passâ€‘through** | Serverâ€‘Sent Events & chunked encoding are forwarded asâ€‘is. |
| **ğŸ—œ Transparent gzip/deflate** | Automatically strips `Contentâ€‘Encoding` so PythonÂ HTTPX & cURL both work. |
| **ğŸ”’ Minimal attack surface** | No business logic, no DB, only HTTP â†’ HTTP(s) with optional token gate. |

---

## ğŸ“‚ Directory structure

```
viyv-llm-gateway/
â”œâ”€ src/
â”‚  â”œâ”€ server.ts          # Hono entry â€“Â routing + proxy logic
â”‚  â”œâ”€ providers.ts       # Provider definitions (base URL & key injection)
â”‚  â””â”€ utils/
â”‚Â Â Â Â Â â””â”€ setHeader.ts    # Safe headerâ€‘setter helper
â”œâ”€ .env.example          # Environment variable template
â”œâ”€ bunfig.toml           # Bun hotâ€‘reload settings (optional)
â”œâ”€ tsconfig.json         # TypeScript options
â”œâ”€ package.json          # nameÂ =Â viyv-llm-gateway
â”œâ”€ Dockerfile            # Lightweight deployment image
â””â”€ LICENSE               # MIT
```

---

## ğŸš€ QuickÂ Start (local)

```bash
# 1. Clone
$ git clone https://github.com/BrainFiber/viyv-llm-gateway.git
$ cd viyv-llm-gateway

# 2. Install deps (Bun 1.1+)
$ bun install

# 3. Configure env
$ cp .env.example .env
$ $EDITOR .env   # paste your provider keys

# 4. Run with hotâ€‘reload
$ bun run dev        # default: http://localhost:3000
```

<details>
<summary>ğŸ— Bun installation</summary>

```bash
# Homebrew (AppleÂ Silicon / Intel)
brew tap oven-sh/bun
brew install bun

# or official script
curl -fsSL https://bun.sh/install | bash
```
</details>

---

## ğŸ³ Docker

```bash
$ docker build -t viyv-llm-gateway .
$ docker run -d --name gateway -p 3000:3000 --env-file .env viyv-llm-gateway
```

---

## ğŸŒ CloudflareÂ Workers (optional)

> Workers use the same `src/server.ts`. Install `wrangler`, add `wrangler.toml`, then:
>
> ```bash
> wrangler deploy
> ```

---

## ğŸ”§ Environment variables

| Name | Required | Example | Purpose |
|------|----------|---------|---------|
| `INTERNAL_TOKEN` | âœ… | `changeme` | Requests **must** include `xâ€‘internal-token` header with this value (set header gate `// app.use` block to enable). |
| `OPENAI_API_KEY` | âš ï¸ when using `openai` | `skâ€‘â€¦` | Auth header for OpenAI. |
| `GOOGLE_API_KEY` | âš ï¸ when using `gemini` | `AIzaâ€¦` | URL query param key for Gemini. |
| `ANTHROPIC_API_KEY` | âš ï¸ when using `anthropic` | `skâ€‘â€¦` | Header `x-api-key` for Anthropic. |

> *Tip*: export them in `.env` and **never** commit that file.

---

## ğŸ Usage examples

### curl via gateway

```bash
curl -X POST http://localhost:3000/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-internal-token: changeme" \
  -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hello"}]}'
```

### OpenAIÂ PythonÂ SDK

```python
import openai

openai.base_url = "http://localhost:3000/openai"
openai.api_key = "changeme"  # internal token or any dummy string

print(openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "ã“ã‚“ã«ã¡ã¯"}]
).choices[0].message.content)
```

---

## â• Adding a new provider

1. Edit `src/providers.ts` and add:
   ```ts
   myllm: {
     base: 'https://api.myâ€‘llm.com',
     inject: (init) => {
       setHeader(init, 'Authorization', `Bearer ${process.env.MYLLM_KEY!}`)
     }
   }
   ```
2. Add `MYLLM_KEY=â€¦` to `.env`.
3. Restart: `/myllm/...` now proxies to your custom endpoint.

---

## ğŸ›  Troubleshooting

| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| `Connection refused â†’ api.openai.com` | Host header forwarded as `localhost` | use the latest **server.ts** (strips host). |
| `DecodingError: incorrect header check` (Python HTTPX) | `Contentâ€‘Encoding: gzip` kept after Bun autoâ€‘decompress | gateway now deletes `content-encoding`. |
| Gateway returns `Unknown provider` | Path prefix not in `PROVIDERS` | Add provider block or use correct prefix. |

---

## ğŸ”®Â Roadmap

- [ ] Rateâ€‘limit & quota middleware (Redis / KV)
- [ ] Prometheus metrics endpoint `/metrics`
- [ ] JWTâ€‘based multiâ€‘tenant auth
- [ ] GitHubÂ Action for automatic Bun â†’ Docker multiâ€‘arch build

---

## ğŸ¤ Contributing

1. Fork & clone
2. `bun install`
3. Create a feature branch (`git checkout -b feat/awesome-feature`)
4. Commit with [ConventionalÂ Commits](https://www.conventionalcommits.org/) style
5. Open a Pull Request â€“Â ensure `bun test` & ESLint pass

All constructive PRs and issues are welcome â™¥

---

## ğŸ“ License

MIT Â© 2025â€¯BrainFiberÂ Inc. See [LICENSE](LICENSE) for details.

---

## ğŸ™Â Acknowledgements

Built with â¤ï¸ using:

- [Bun](https://bun.sh)
- [Hono](https://hono.dev)
- Provider SDKs & public APIs of OpenAI, Google, Anthropic.

> This project is **not** affiliated with or endorsed by the providers above.

